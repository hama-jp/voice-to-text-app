#!/usr/bin/env python3
"""
æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆæ ¡æ­£ãƒ»èª¤å­—è¨‚æ­£ã‚·ã‚¹ãƒ†ãƒ 
è»½é‡ã‹ã¤é«˜å“è³ªãªæ–‡ç« æ”¹å–„ã‚’å®Ÿç¾
"""

import re
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import jaconv
import time

class JapaneseTextCorrector:
    """æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆæ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, use_llm=True, model_name="Qwen/Qwen3-8B"):
        """
        åˆæœŸåŒ–
        
        Args:
            use_llm (bool): LLMã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
            model_name (str): ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«åï¼ˆæ¨å¥¨: Qwen/Qwen3-8Bï¼‰
        """
        self.use_llm = use_llm
        self.model_name = model_name
        self.llm_pipeline = None
        
        # åŸºæœ¬çš„ãªèª¤å­—è¨‚æ­£è¾æ›¸
        self.correction_dict = {
            # åŠ©è©ã®èª¤ç”¨
            "ã“ã‚“ã«ã¡ã‚": "ã“ã‚“ã«ã¡ã¯",
            "ã“ã‚“ã°ã‚“ã‚": "ã“ã‚“ã°ã‚“ã¯",
            "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ": "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ",
            
            # ã‚ˆãã‚ã‚‹èª¤å­—
            "é›°æ„æ°—": "é›°å›²æ°—",
            "ã‚·ãƒ¥ãƒŸãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³": "ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³",
            "ã‚³ãƒŸãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³": "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³",
            "ã‚·ãƒŸãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³": "ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³",
            "ã‚¢ã‚¯ã‚»ã‚µãƒª": "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼",
            "ãƒãƒƒãƒ†ãƒª": "ãƒãƒƒãƒ†ãƒªãƒ¼",
            
            # æ•¬èªã®èª¤ç”¨
            "ã•ã›ã¦ã„ãŸã ã": "ã„ãŸã—ã¾ã™",
            "ãŠç–²ã‚Œæ§˜ã§ã—ãŸ": "ãŠç–²ã‚Œã•ã¾ã§ã—ãŸ",
            
            # æ¼¢å­—ã®èª¤ç”¨
            "å€‹å®¢": "é¡§å®¢",
            "æ¬¡å¼Ÿ": "æ¬¡ç¬¬",
            "åŒå¿—": "åŒå£«",
            "æ„å¿—": "æ„æ€",
        }
        
        # æ–‡å­—ç¨®çµ±ä¸€ãƒ«ãƒ¼ãƒ«
        self.normalization_rules = [
            # æ•°å­—ã®å…¨è§’â†’åŠè§’çµ±ä¸€
            (r'[ï¼-ï¼™]', lambda m: str(ord(m.group()) - ord('ï¼'))),
            # è‹±å­—ã®å…¨è§’â†’åŠè§’çµ±ä¸€
            (r'[ï¼¡-ï¼ºï½-ï½š]', lambda m: chr(ord(m.group()) - ord('ï¼¡') + ord('A')) if 'ï¼¡' <= m.group() <= 'ï¼º' else chr(ord(m.group()) - ord('ï½') + ord('a'))),
            # è¨˜å·ã®çµ±ä¸€
            (r'ï¼', '!'),
            (r'ï¼Ÿ', '?'),
            (r'ï¼', '.'),
            (r'ï¼Œ', ','),
        ]
        
        if self.use_llm:
            self._initialize_llm()
    
    def _initialize_llm(self):
        """LLMãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ï¼ˆQwen2.5-7B-Instruct 4bité‡å­åŒ–ï¼‰"""
        try:
            print(f"ğŸ”„ LLMãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­: {self.model_name}")
            start_time = time.time()
            
            # GPUåˆ©ç”¨å¯èƒ½æ€§ç¢ºèª
            device = "cuda" if torch.cuda.is_available() else "cpu"
            
            # 4bité‡å­åŒ–è¨­å®š
            from transformers import BitsAndBytesConfig
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
            from transformers import AutoTokenizer, AutoModelForCausalLM
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆ4bité‡å­åŒ–ï¼‰
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                quantization_config=quantization_config if device == "cuda" else None,
                device_map="auto" if device == "cuda" else None,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            load_time = time.time() - start_time
            print(f"âœ… LLMãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº† ({load_time:.1f}ç§’)")
            
            # GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
            if device == "cuda":
                memory_used = torch.cuda.memory_allocated() / 1024**3
                print(f"ğŸ“Š GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_used:.1f}GB")
            
        except Exception as e:
            print(f"âš ï¸  LLMãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—: {e}")
            print("ğŸ“ åŸºæœ¬çš„ãªæ–‡å­—åˆ—å‡¦ç†ã®ã¿ã§å‹•ä½œã—ã¾ã™")
            self.use_llm = False
            self.model = None
            self.tokenizer = None
    
    def basic_correction(self, text):
        """åŸºæœ¬çš„ãªèª¤å­—è¨‚æ­£ã¨æ­£è¦åŒ–"""
        
        # æ–‡å­—ç¨®æ­£è¦åŒ–
        corrected = text
        for pattern, replacement in self.normalization_rules:
            if callable(replacement):
                corrected = re.sub(pattern, replacement, corrected)
            else:
                corrected = corrected.replace(pattern, replacement)
        
        # ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠæ­£è¦åŒ–
        corrected = jaconv.normalize(corrected)
        
        # è¾æ›¸ãƒ™ãƒ¼ã‚¹èª¤å­—è¨‚æ­£
        for wrong, correct in self.correction_dict.items():
            corrected = corrected.replace(wrong, correct)
        
        # é€£ç¶šã™ã‚‹å¥èª­ç‚¹ã®æ•´ç†
        corrected = re.sub(r'[ã€ã€‚]{2,}', 'ã€‚', corrected)
        corrected = re.sub(r'[ï¼Œï¼]{2,}', 'ã€‚', corrected)
        
        # ä¸è¦ãªç©ºç™½ã®é™¤å»
        corrected = re.sub(r'\s+', ' ', corrected)
        corrected = corrected.strip()
        
        return corrected
    
    def llm_correction(self, text, max_new_tokens=80):
        """Qwen2.5-7B-Instructã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªèª¤å­—è¨‚æ­£"""
        
        if not self.use_llm or not hasattr(self, 'model') or not self.model:
            return text
        
        try:
            # å˜ç´”ãªæŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæ—¥æœ¬èªç¢ºå®Ÿå‡ºåŠ›ï¼‰
            prompt = f"""ä»¥ä¸‹ã®æ—¥æœ¬èªæ–‡ç« ã®èª¤å­—è„±å­—ã‚’ä¿®æ­£ã—ã¦ã€æ­£ã—ã„æ—¥æœ¬èªã«æ ¡æ­£ã—ã¦ãã ã•ã„ã€‚

å…ƒã®æ–‡ç« : {text}
æ ¡æ­£å¾Œ:"""
            
            text_input = prompt
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(
                text_input,
                return_tensors="pt",
                truncation=True,
                max_length=512
            )
            
            # GPUè¨­å®š
            device = "cuda" if torch.cuda.is_available() else "cpu"
            if device == "cuda":
                inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # ç”Ÿæˆè¨­å®šï¼ˆç¢ºå®Ÿæ€§é‡è¦–ãƒ»æ—¥æœ¬èªå‡ºåŠ›ï¼‰
            generation_config = {
                "max_new_tokens": max_new_tokens,
                "temperature": 0.1,  # ã‚ˆã‚Šä½æ¸©åº¦ã§ç¢ºå®Ÿãªæ—¥æœ¬èªå‡ºåŠ›
                "top_p": 0.8,
                "do_sample": True,
                "repetition_penalty": 1.1,
                "pad_token_id": self.tokenizer.pad_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "use_cache": True
            }
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **generation_config
                )
            
            # çµæœãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆå…¥åŠ›éƒ¨åˆ†ã‚’é™¤å»ï¼‰
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )
            
            # æ ¡æ­£çµæœã®æŠ½å‡ºã¨æ¸…ç†
            corrected_text = generated_text.strip()
            
            # <think>ã‚¿ã‚°ã¨ãã®å†…å®¹ã‚’é™¤å»
            import re
            corrected_text = re.sub(r'<think>.*?</think>', '', corrected_text, flags=re.DOTALL)
            corrected_text = corrected_text.replace('<think>', '').replace('</think>', '')
            
            # ä¸è¦ãªå‰ç½®è©ã‚„è¨˜å·ã‚’é™¤å»
            corrected_text = corrected_text.replace("ä¿®æ­£å¾Œ:", "").strip()
            corrected_text = corrected_text.replace("æ ¡æ­£å¾Œ:", "").strip()
            corrected_text = corrected_text.replace("ã€Œ", "").replace("ã€", "")
            corrected_text = corrected_text.split("\n")[0].strip()  # æœ€åˆã®è¡Œã®ã¿
            
            # ç©ºã®çµæœã‚„å…ƒãƒ†ã‚­ã‚¹ãƒˆã¨åŒã˜å ´åˆ
            if not corrected_text or corrected_text == text:
                return text
            
            return corrected_text
                
        except Exception as e:
            print(f"âš ï¸  LLMè¨‚æ­£ã‚¨ãƒ©ãƒ¼: {e}")
            return text
    
    def correct_text(self, text, use_advanced=True):
        """
        ç·åˆçš„ãªãƒ†ã‚­ã‚¹ãƒˆæ ¡æ­£
        
        Args:
            text (str): æ ¡æ­£å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
            use_advanced (bool): LLMã«ã‚ˆã‚‹é«˜åº¦ãªæ ¡æ­£ã‚’ä½¿ç”¨ã™ã‚‹ã‹
            
        Returns:
            dict: æ ¡æ­£çµæœã¨æƒ…å ±
        """
        start_time = time.time()
        
        # å…¥åŠ›æ¤œè¨¼
        if not text or not text.strip():
            return {
                "original": text,
                "corrected": text,
                "changes": [],
                "processing_time": 0,
                "method": "no_processing"
            }
        
        # åŸºæœ¬æ ¡æ­£
        basic_corrected = self.basic_correction(text)
        changes = []
        
        if basic_corrected != text:
            changes.append("åŸºæœ¬çš„ãªèª¤å­—è¨‚æ­£ãƒ»æ­£è¦åŒ–")
        
        # é«˜åº¦ãªæ ¡æ­£ï¼ˆLLMä½¿ç”¨ï¼‰
        final_corrected = basic_corrected
        if use_advanced and self.use_llm:
            llm_corrected = self.llm_correction(basic_corrected)
            if llm_corrected and llm_corrected != basic_corrected:
                final_corrected = llm_corrected
                changes.append("LLMã«ã‚ˆã‚‹é«˜åº¦ãªæ ¡æ­£")
        
        processing_time = time.time() - start_time
        
        return {
            "original": text,
            "corrected": final_corrected,
            "changes": changes,
            "processing_time": processing_time,
            "method": "llm" if use_advanced and self.use_llm else "basic"
        }
    
    def test_correction(self):
        """æ ¡æ­£æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
        
        test_cases = [
            "ä»Šæ—¥ã‚ã¨ã¦ã‚‚ã‚ˆã„å¤©æ°—ã§ã™ã­ã€‚",
            "ä¼šè­°ã®æ¬¡å¼Ÿã¯æ˜æ—¥ã®åˆå¾Œï¼“æ™‚ã‹ã‚‰ã§ã™ã€‚",
            "ã“ã®å•†å“ã®å€‹å®¢æº€è¶³åº¦ã¯é«˜ã„ã§ã™ã€‚",
            "ã‚·ãƒ¥ãƒŸãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®çµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
            "ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼ï¼ï¼",
            "ã“ã‚“ã«ã¡ã‚ã€ã„ã‹ãŒãŠéã”ã—ã§ã™ã‹ï¼Ÿï¼Ÿï¼Ÿ",
        ]
        
        print("ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆæ ¡æ­£æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ")
        print("=" * 50)
        
        for i, test_text in enumerate(test_cases, 1):
            print(f"\n--- ãƒ†ã‚¹ãƒˆ {i} ---")
            result = self.correct_text(test_text)
            
            print(f"ä¿®æ­£å‰: {result['original']}")
            print(f"ä¿®æ­£å¾Œ: {result['corrected']}")
            print(f"å¤‰æ›´ç‚¹: {', '.join(result['changes']) if result['changes'] else 'ãªã—'}")
            print(f"å‡¦ç†æ™‚é–“: {result['processing_time']:.3f}ç§’")
            print(f"ä½¿ç”¨æ‰‹æ³•: {result['method']}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆæ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
    
    # è»½é‡ç‰ˆã§åˆæœŸåŒ–ï¼ˆå•é¡ŒãŒã‚ã‚Œã°åŸºæœ¬ç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    corrector = JapaneseTextCorrector(use_llm=True, model_name="rinna/japanese-gpt-neox-small")
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    corrector.test_correction()
    
    print(f"\nâœ… åˆæœŸåŒ–å®Œäº†")
    print(f"ğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print(f"   corrector = JapaneseTextCorrector()")
    print(f"   result = corrector.correct_text('æ ¡æ­£ã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆ')")

if __name__ == "__main__":
    main()