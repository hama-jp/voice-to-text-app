#!/usr/bin/env python3
"""
æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆæ ¡æ­£ãƒ»èª¤å­—è¨‚æ­£ã‚·ã‚¹ãƒ†ãƒ 
è»½é‡ã‹ã¤é«˜å“è³ªãªæ–‡ç« æ”¹å–„ã‚’å®Ÿç¾
"""

import re
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import jaconv
import time

class JapaneseTextCorrector:
    """æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆæ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """
        åˆæœŸåŒ–
        """
        self.models = {}
        self.tokenizers = {}
        
        # éŸ³å£°èªè­˜èª¤èªè­˜ä¿®æ­£è¾æ›¸ï¼ˆæŠ€è¡“çš„ã‚¨ãƒ©ãƒ¼ã®ã¿ï¼‰
        self.correction_dict = {
            # åŠ©è©ã®éŸ³å£°èªè­˜èª¤èªè­˜
            "ã“ã‚“ã«ã¡ã‚": "ã“ã‚“ã«ã¡ã¯",
            "ã“ã‚“ã°ã‚“ã‚": "ã“ã‚“ã°ã‚“ã¯",
            "ãã‚‡ã†ã‚": "ä»Šæ—¥ã¯",
            "ã‚ã—ãŸã‚": "æ˜æ—¥ã¯",
            "ãã‚Œã‚": "ãã‚Œã¯",
            "ã“ã‚Œã‚": "ã“ã‚Œã¯",
            "ã‚ã‚Œã‚": "ã‚ã‚Œã¯",
            "ã©ã‚Œã‚": "ã©ã‚Œã¯",
            "ã ã‚Œã‚": "èª°ã¯",
            "ãªã«ã‚": "ä½•ã¯",
            "ã„ã¤ã‚": "ã„ã¤ã¯",
            "ã©ã“ã‚": "ã©ã“ã¯",
            "ã‚ãŸã—ã‚": "ç§ã¯",
            "ã‚ãªãŸã‚": "ã‚ãªãŸã¯",
            "ã‹ã‚Œã‚": "å½¼ã¯",
            "ã‹ã®ã˜ã‚‡ã‚": "å½¼å¥³ã¯",
            
            # åŒéŸ³ç•°ç¾©èªã®å¤‰æ›ãƒŸã‚¹ï¼ˆéŸ³å£°èªè­˜ç‰¹æœ‰ï¼‰
            "å€‹å®¢": "é¡§å®¢",
            "æ¬¡å¼Ÿ": "æ¬¡ç¬¬",
            "åŒå¿—": "åŒå£«",
            "é›°æ„æ°—": "é›°å›²æ°—",
            "ã‚·ãƒ¥ãƒŸãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³": "ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³",
            "ã‚³ãƒŸãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³": "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³",
            "ã‚·ãƒŸãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³": "ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³",
            
            # ã‚«ã‚¿ã‚«ãƒŠèªã®é•·éŸ³è¨˜å·çœç•¥ï¼ˆéŸ³å£°èªè­˜ã§ã‚ˆãç™ºç”Ÿï¼‰
            "ã‚¢ã‚¯ã‚»ã‚µãƒª": "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼",
            "ãƒãƒƒãƒ†ãƒª": "ãƒãƒƒãƒ†ãƒªãƒ¼",
            "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿": "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼",
            "ãƒ—ãƒªãƒ³ã‚¿": "ãƒ—ãƒªãƒ³ã‚¿ãƒ¼",
            "ã‚¹ã‚­ãƒ£ãƒŠ": "ã‚¹ã‚­ãƒ£ãƒŠãƒ¼",
            "ãƒ¢ãƒ‹ã‚¿": "ãƒ¢ãƒ‹ã‚¿ãƒ¼",
            "ãƒ¦ãƒ¼ã‚¶": "ãƒ¦ãƒ¼ã‚¶ãƒ¼",
            "ã‚µãƒ¼ãƒ": "ã‚µãƒ¼ãƒãƒ¼",
            "ãƒ–ãƒ©ã‚¦ã‚¶": "ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¼",
            "ãƒ•ã‚©ãƒ«ãƒ€": "ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼",
            
            # æ•°å­—ã®å…¨è§’â†’åŠè§’çµ±ä¸€ï¼ˆéŸ³å£°èªè­˜ã§å…¨è§’ã«ãªã‚ŠãŒã¡ï¼‰
            "ï¼‘": "1", "ï¼’": "2", "ï¼“": "3", "ï¼”": "4", "ï¼•": "5",
            "ï¼–": "6", "ï¼—": "7", "ï¼˜": "8", "ï¼™": "9", "ï¼": "0",
            
            # éŸ³å£°èªè­˜ã§ã²ã‚‰ãŒãªåŒ–ã•ã‚ŒãŸä¸€èˆ¬çš„ãªå˜èª
            "ãã‚‡ã†": "ä»Šæ—¥",
            "ã‚ã—ãŸ": "æ˜æ—¥",
            "ãã®ã†": "æ˜¨æ—¥",
            "ã¦ã‚“ã": "å¤©æ°—",
            "ã˜ã‹ã‚“": "æ™‚é–“",
            "ã°ã—ã‚‡": "å ´æ‰€",
            "ã‹ã„ã—ã‚ƒ": "ä¼šç¤¾",
            "ã—ã”ã¨": "ä»•äº‹",
            "ãŒã£ã“ã†": "å­¦æ ¡",
            "ã§ã‚“ã‚": "é›»è©±",
            "ã»ã‚“": "æœ¬",
            "ã¦ã‚Œã³": "ãƒ†ãƒ¬ãƒ“",
            "ã±ãã“ã‚“": "ãƒ‘ã‚½ã‚³ãƒ³",
            "ã‘ã„ãŸã„": "æºå¸¯",
            
            # é€ã‚Šä»®åã®çµ±ä¸€ï¼ˆéŸ³å£°èªè­˜ã§é€ã‚Šä»®åãŒä»˜ããŒã¡ï¼‰
            "å£²ã‚Šä¸Šã’": "å£²ä¸Š",
            "å–ã‚Šå¼•ã": "å–å¼•",
            "ç”³ã—è¾¼ã¿": "ç”³è¾¼",
            "æ‰“ã¡åˆã‚ã›": "æ‰“åˆã›",
            "è¦‹ç©ã‚‚ã‚Š": "è¦‹ç©ã‚Š",
            "æ‰‹ç¶šã": "æ‰‹ç¶š",
            "å—ã‘ä»˜ã‘": "å—ä»˜",
            "æŒ¯ã‚Šè¾¼ã¿": "æŒ¯è¾¼",
        }
        
        # æ–‡å­—ç¨®çµ±ä¸€ãƒ«ãƒ¼ãƒ«
        self.normalization_rules = [
            # æ•°å­—ã®å…¨è§’â†’åŠè§’çµ±ä¸€
            (r'[ï¼-ï¼™]', lambda m: str(ord(m.group()) - ord('ï¼'))),
            # è‹±å­—ã®å…¨è§’â†’åŠè§’çµ±ä¸€
            (r'[ï¼¡-ï¼ºï½-ï½š]', lambda m: chr(ord(m.group()) - ord('ï¼¡') + ord('A')) if 'ï¼¡' <= m.group() <= 'ï¼º' else chr(ord(m.group()) - ord('ï½') + ord('a'))),
            # è¨˜å·ã®çµ±ä¸€
            (r'ï¼', '!'),
            (r'ï¼Ÿ', '?'),
            (r'ï¼', '.'),
            (r'ï¼Œ', ','),
        ]
    
    def load_model(self, model_name):
        """LLMãƒ¢ãƒ‡ãƒ«ã®å‹•çš„èª­ã¿è¾¼ã¿"""
        if model_name in self.models:
            return

        try:
            print(f"ğŸ”„ LLMãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­: {model_name}")
            start_time = time.time()
            
            device = "cuda" if torch.cuda.is_available() else "cpu"
            
            quantization_config = None
            if "Qwen" in model_name and device == "cuda":
                from transformers import BitsAndBytesConfig
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
            
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True
            )
            
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map="auto" if device == "cuda" else None,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            self.models[model_name] = model
            self.tokenizers[model_name] = tokenizer
            
            load_time = time.time() - start_time
            print(f"âœ… LLMãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº† ({load_time:.1f}ç§’): {model_name}")
            
            if device == "cuda":
                memory_used = torch.cuda.memory_allocated() / 1024**3
                print(f"ğŸ“Š GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_used:.1f}GB")
            
        except Exception as e:
            print(f"âš ï¸  LLMãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—: {e}")
            if model_name in self.models:
                del self.models[model_name]
            if model_name in self.tokenizers:
                del self.tokenizers[model_name]

    def basic_correction(self, text):
        """åŸºæœ¬çš„ãªèª¤å­—è¨‚æ­£ã¨æ­£è¦åŒ–"""
        
        # æ–‡å­—ç¨®æ­£è¦åŒ–
        corrected = text
        for pattern, replacement in self.normalization_rules:
            if callable(replacement):
                corrected = re.sub(pattern, replacement, corrected)
            else:
                corrected = corrected.replace(pattern, replacement)
        
        # ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠæ­£è¦åŒ–
        corrected = jaconv.normalize(corrected)
        
        # è¾æ›¸ãƒ™ãƒ¼ã‚¹èª¤å­—è¨‚æ­£
        for wrong, correct in self.correction_dict.items():
            corrected = corrected.replace(wrong, correct)
        
        # é€£ç¶šã™ã‚‹å¥èª­ç‚¹ã®æ•´ç†
        corrected = re.sub(r'[ã€ã€‚]{2,}', 'ã€‚', corrected)
        corrected = re.sub(r'[ï¼Œï¼]{2,}', 'ã€‚', corrected)
        
        # ä¸è¦ãªç©ºç™½ã®é™¤å»
        corrected = re.sub(r'\s+', ' ', corrected)
        corrected = corrected.strip()
        
        return corrected
    
    def llm_correction(self, text, model_name, max_new_tokens=80):
        """æŒ‡å®šã•ã‚ŒãŸLLMã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªèª¤å­—è¨‚æ­£"""
        
        if model_name not in self.models:
            self.load_model(model_name)
            if model_name not in self.models:
                print(f"âš ï¸  LLMãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€æ ¡æ­£ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™: {model_name}")
                return text

        model = self.models[model_name]
        tokenizer = self.tokenizers[model_name]
        
        try:
            # é«˜ç²¾åº¦æ—¥æœ¬èªæ ¡æ­£ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            prompt = f'''ã‚ãªãŸã¯æ—¥æœ¬èªæ ¡æ­£ã®å°‚é–€å®¶ã§ã™ã€‚éŸ³å£°èªè­˜ã§ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã€è‡ªç„¶ã§æ­£ç¢ºãªæ—¥æœ¬èªã«æ ¡æ­£ã—ã¦ãã ã•ã„ã€‚

ã€æ ¡æ­£é …ç›®ã€‘
1. èª¤å­—è„±å­—ã®ä¿®æ­£ï¼ˆåŒéŸ³ç•°ç¾©èªã€å¤‰æ›ãƒŸã‚¹ï¼‰
2. åŠ©è©ã®é©åˆ‡ãªä½¿ã„åˆ†ã‘ï¼ˆã¯/ãŒã€ã‚’/ã«ã€ã§/ã¨ç­‰ï¼‰
3. æ•¬èªãƒ»ä¸å¯§èªã®è‡ªç„¶ãªè¡¨ç¾
4. æ–‡ç« æ§‹é€ ã®æ”¹å–„ï¼ˆä¸»èªè¿°èªã®å¯¾å¿œã€ä¿®é£¾é–¢ä¿‚ï¼‰
5. èª­ç‚¹ãƒ»å¥ç‚¹ã®é©åˆ‡ãªé…ç½®

ã€å³æ ¼ãªãƒ«ãƒ¼ãƒ«ã€‘
- å…ƒã®æ–‡ç« ã®æ„å‘³ãƒ»æ„å›³ã‚’çµ¶å¯¾ã«å¤‰æ›´ã—ãªã„
- äº‹å®Ÿãƒ»æ•°å€¤ãƒ»å›ºæœ‰åè©ã¯åŸæ–‡é€šã‚Šä¿æŒ
- éŸ³å£°èªè­˜ç‰¹æœ‰ã®èª¤èªè­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è€ƒæ…®ã™ã‚‹
- å£èªçš„è¡¨ç¾ã¯é©åº¦ã«æ›¸ãè¨€è‘‰ã«èª¿æ•´
- æ ¡æ­£çµæœã®ã¿ã‚’ç°¡æ½”ã«å‡ºåŠ›ã™ã‚‹

ã€éŸ³å£°èªè­˜èª¤èªè­˜ã®ä¾‹ã€‘
å…¥åŠ›: ãã‚‡ã†ã‚ã¨ã¦ã‚‚ã„ã„ã¦ã‚“ãã§ã™ã­
æ ¡æ­£: ä»Šæ—¥ã¯ã¨ã¦ã‚‚è‰¯ã„å¤©æ°—ã§ã™ã­

å…¥åŠ›: ã‹ã„ãã®ã—ã ã„ã‚ã‚ã—ãŸã®ã”ã”ã•ã‚“ã˜ã‹ã‚‰ã§ã™
æ ¡æ­£: ä¼šè­°ã®æ¬¡ç¬¬ã¯æ˜æ—¥ã®åˆå¾Œï¼“æ™‚ã‹ã‚‰ã§ã™

å…¥åŠ›: ã“ã®ã—ã‚‡ã†ã²ã‚“ã®ã“ãã‚ƒãã¾ã‚“ããã©ã‚ãŸã‹ã„ã§ã™
æ ¡æ­£: ã“ã®å•†å“ã®é¡§å®¢æº€è¶³åº¦ã¯é«˜ã„ã§ã™

ã€æ ¡æ­£å¯¾è±¡ã€‘
å…¥åŠ›: {text}
æ ¡æ­£:'''
            
            text_input = prompt
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = tokenizer(
                text_input,
                return_tensors="pt",
                truncation=True,
                max_length=512
            )
            
            # GPUè¨­å®š
            device = "cuda" if torch.cuda.is_available() else "cpu"
            if device == "cuda":
                inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # ç”Ÿæˆè¨­å®šï¼ˆç¢ºå®Ÿæ€§é‡è¦–ãƒ»æ—¥æœ¬èªå‡ºåŠ›ï¼‰
            generation_config = {
                "max_new_tokens": max_new_tokens,
                "temperature": 0.1,
                "top_p": 0.8,
                "do_sample": True,
                "repetition_penalty": 1.1,
                "pad_token_id": tokenizer.pad_token_id,
                "eos_token_id": tokenizer.eos_token_id,
                "use_cache": True
            }
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    **generation_config
                )
            
            # çµæœãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆå…¥åŠ›éƒ¨åˆ†ã‚’é™¤å»ï¼‰
            generated_text = tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )
            
            # æ ¡æ­£çµæœã®æŠ½å‡ºã¨æ¸…ç†
            corrected_text = generated_text.strip()
            
            # <think>ã‚¿ã‚°ã¨ãã®å†…å®¹ã‚’é™¤å»
            import re
            corrected_text = re.sub(r'<think>.*?</think>', '', corrected_text, flags=re.DOTALL)
            corrected_text = corrected_text.replace('<think>', '').replace('</think>', '')
            
            # ä¸è¦ãªå‰ç½®è©ã‚„è¨˜å·ã‚’é™¤å»
            corrected_text = corrected_text.replace("ä¿®æ­£å¾Œ:", "").strip()
            corrected_text = corrected_text.replace("æ ¡æ­£å¾Œ:", "").strip()
            corrected_text = corrected_text.replace("ã€Œ", "").replace("ã€", "")
            corrected_text = corrected_text.split("\n")[0].strip()  # æœ€åˆã®è¡Œã®ã¿
            
            # ç©ºã®çµæœã‚„å…ƒãƒ†ã‚­ã‚¹ãƒˆã¨åŒã˜å ´åˆ
            if not corrected_text or corrected_text == text:
                return text
            
            return corrected_text
                
        except Exception as e:
            print(f"âš ï¸  LLMè¨‚æ­£ã‚¨ãƒ©ãƒ¼: {e}")
            return text
    
    def correct_text(self, text, model_name="rinna/japanese-gpt-neox-small"):
        """
        ç·åˆçš„ãªãƒ†ã‚­ã‚¹ãƒˆæ ¡æ­£
        
        Args:
            text (str): æ ¡æ­£å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
            model_name (str): ä½¿ç”¨ã™ã‚‹LLMã®ãƒ¢ãƒ‡ãƒ«å
            
        Returns:
            dict: æ ¡æ­£çµæœã¨æƒ…å ±
        """
        start_time = time.time()
        
        # å…¥åŠ›æ¤œè¨¼
        if not text or not text.strip():
            return {
                "original": text,
                "corrected": text,
                "changes": [],
                "processing_time": 0,
                "method": "no_processing"
            }
        
        # åŸºæœ¬æ ¡æ­£
        basic_corrected = self.basic_correction(text)
        changes = []
        
        if basic_corrected != text:
            changes.append("åŸºæœ¬çš„ãªèª¤å­—è¨‚æ­£ãƒ»æ­£è¦åŒ–")
        
        # é«˜åº¦ãªæ ¡æ­£ï¼ˆLLMä½¿ç”¨ï¼‰
        final_corrected = basic_corrected
        if model_name:
            llm_corrected = self.llm_correction(basic_corrected, model_name)
            if llm_corrected and llm_corrected != basic_corrected:
                final_corrected = llm_corrected
                changes.append(f"LLMã«ã‚ˆã‚‹é«˜åº¦ãªæ ¡æ­£ ({model_name})")
        
        processing_time = time.time() - start_time
        
        return {
            "original": text,
            "corrected": final_corrected,
            "changes": changes,
            "processing_time": processing_time,
            "method": f"llm ({model_name})" if model_name else "basic"
        }
    
    def test_correction(self):
        """æ ¡æ­£æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
        
        test_cases = [
            "ä»Šæ—¥ã‚ã¨ã¦ã‚‚ã‚ˆã„å¤©æ°—ã§ã™ã­ã€‚",
            "ä¼šè­°ã®æ¬¡å¼Ÿã¯æ˜æ—¥ã®åˆå¾Œï¼“æ™‚ã‹ã‚‰ã§ã™ã€‚",
            "ã“ã®å•†å“ã®å€‹å®¢æº€è¶³åº¦ã¯é«˜ã„ã§ã™ã€‚",
            "ã‚·ãƒ¥ãƒŸãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®çµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
            "ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼ï¼ï¼",
            "ã“ã‚“ã«ã¡ã‚ã€ã„ã‹ãŒãŠéã”ã—ã§ã™ã‹ï¼Ÿï¼Ÿï¼Ÿ",
        ]
        
        print("ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆæ ¡æ­£æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ")
        print("=" * 50)
        
        for i, test_text in enumerate(test_cases, 1):
            print(f"\n--- ãƒ†ã‚¹ãƒˆ {i} ---")
            result = self.correct_text(test_text)
            
            print(f"ä¿®æ­£å‰: {result['original']}")
            print(f"ä¿®æ­£å¾Œ: {result['corrected']}")
            print(f"å¤‰æ›´ç‚¹: {', '.join(result['changes']) if result['changes'] else 'ãªã—'}")
            print(f"å‡¦ç†æ™‚é–“: {result['processing_time']:.3f}ç§’")
            print(f"ä½¿ç”¨æ‰‹æ³•: {result['method']}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆæ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
    
    # è»½é‡ç‰ˆã§åˆæœŸåŒ–ï¼ˆå•é¡ŒãŒã‚ã‚Œã°åŸºæœ¬ç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    corrector = JapaneseTextCorrector(use_llm=True, model_name="rinna/japanese-gpt-neox-small")
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    corrector.test_correction()
    
    print(f"\nâœ… åˆæœŸåŒ–å®Œäº†")
    print(f"ğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print(f"   corrector = JapaneseTextCorrector()")
    print(f"   result = corrector.correct_text('æ ¡æ­£ã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆ')")

if __name__ == "__main__":
    main()