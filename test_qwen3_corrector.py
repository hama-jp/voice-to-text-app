#!/usr/bin/env python3
"""
Qwen3-8Bãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸæ—¥æœ¬èªæ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ
4bité‡å­åŒ–ã«ã‚ˆã‚‹é«˜åŠ¹ç‡å®Ÿè£…
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import time
import gc

class Qwen3TextCorrector:
    """Qwen3-8Bã‚’ä½¿ç”¨ã—ãŸæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆæ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, model_name="Qwen/Qwen3-8B-Instruct"):
        """
        åˆæœŸåŒ–
        
        Args:
            model_name (str): ä½¿ç”¨ã™ã‚‹Qwen3ãƒ¢ãƒ‡ãƒ«å
        """
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸ¤– Qwen3æ—¥æœ¬èªæ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–: {model_name}")
        self._initialize_model()
    
    def _initialize_model(self):
        """4bité‡å­åŒ–ã§Qwen3ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–"""
        try:
            print("ğŸ”„ Qwen3ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
            start_time = time.time()
            
            # 4bité‡å­åŒ–è¨­å®šï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡æœ€é©åŒ–ï¼‰
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_quant_storage=torch.uint8
            )
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
            print("   ğŸ“š ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                use_fast=True
            )
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆ4bité‡å­åŒ– + æœ€é©åŒ–è¨­å®šï¼‰
            print("   ğŸ§  Qwen3ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                quantization_config=quantization_config,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                use_flash_attention_2=False,  # å®‰å®šæ€§é‡è¦–
                attn_implementation="eager"   # ç¢ºå®Ÿãªå®Ÿè£…
            )
            
            # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            self.model.eval()
            
            load_time = time.time() - start_time
            print(f"âœ… Qwen3ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº† ({load_time:.1f}ç§’)")
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
            if self.device == "cuda":
                memory_used = torch.cuda.memory_allocated() / 1024**3
                print(f"   GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_used:.1f}GB")
            
            return True
            
        except Exception as e:
            print(f"âŒ Qwen3ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ’¡ ä»£æ›¿æ¡ˆ: ã‚ˆã‚Šè»½é‡ãªãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™ã‹ã€CPUãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")
            self.model = None
            self.tokenizer = None
            return False
    
    def correct_text(self, text, max_new_tokens=80):
        """
        Qwen3ã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆæ ¡æ­£
        
        Args:
            text (str): æ ¡æ­£å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
            max_new_tokens (int): æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°
            
        Returns:
            dict: æ ¡æ­£çµæœ
        """
        if not self.model or not self.tokenizer:
            return {
                "original": text,
                "corrected": text,
                "success": False,
                "error": "ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
            }
        
        start_time = time.time()
        
        try:
            # Qwen3ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆç°¡æ½”ã§åŠ¹æœçš„ï¼‰
            messages = [
                {
                    "role": "system", 
                    "content": "ã‚ãªãŸã¯æ—¥æœ¬èªæ–‡ç« æ ¡æ­£ã®å°‚é–€å®¶ã§ã™ã€‚èª¤å­—è„±å­—ã®ä¿®æ­£ã€é©åˆ‡ãªæ¼¢å­—ã®ä½¿ç”¨ã€è‡ªç„¶ãªè¡¨ç¾ã¸ã®æ”¹å–„ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚ä¿®æ­£å¾Œã®æ–‡ç« ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
                },
                {
                    "role": "user", 
                    "content": f"ä»¥ä¸‹ã®æ–‡ç« ã‚’æ ¡æ­£ã—ã¦ãã ã•ã„ï¼š\n{text}"
                }
            ]
            
            # ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨
            text_input = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆåŠ¹ç‡çš„ãªè¨­å®šï¼‰
            inputs = self.tokenizer(
                text_input,
                return_tensors="pt",
                truncation=True,
                max_length=512,  # å…¥åŠ›é•·åˆ¶é™
                padding=False
            )
            
            if self.device == "cuda":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ç”Ÿæˆè¨­å®šï¼ˆå“è³ªã¨é€Ÿåº¦ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
            generation_config = {
                "max_new_tokens": max_new_tokens,
                "temperature": 0.2,  # ä½æ¸©åº¦ã§ç¢ºå®Ÿæ€§é‡è¦–
                "top_p": 0.85,
                "do_sample": True,
                "repetition_penalty": 1.05,
                "pad_token_id": self.tokenizer.pad_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "use_cache": True
            }
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **generation_config
                )
            
            # çµæœãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],  # å…¥åŠ›éƒ¨åˆ†ã‚’é™¤å»
                skip_special_tokens=True
            )
            
            # æ ¡æ­£çµæœã®æŠ½å‡ºã¨æ¸…ç†
            corrected_text = generated_text.strip()
            
            # ä¸è¦ãªå‰ç½®è©ã‚„è¨˜å·ã‚’é™¤å»
            corrected_text = corrected_text.replace("ä¿®æ­£å¾Œ:", "").strip()
            corrected_text = corrected_text.replace("æ ¡æ­£å¾Œ:", "").strip()
            corrected_text = corrected_text.replace("ã€Œ", "").replace("ã€", "")
            corrected_text = corrected_text.split("\n")[0].strip()  # æœ€åˆã®è¡Œã®ã¿
            
            # ç©ºã®çµæœã‚„å…ƒãƒ†ã‚­ã‚¹ãƒˆã¨åŒã˜å ´åˆ
            if not corrected_text or corrected_text == text:
                corrected_text = text
            
            processing_time = time.time() - start_time
            
            return {
                "original": text,
                "corrected": corrected_text,
                "success": True,
                "processing_time": processing_time,
                "changed": corrected_text != text,
                "model": "Qwen3-8B-Instruct-4bit"
            }
            
        except Exception as e:
            return {
                "original": text,
                "corrected": text,
                "success": False,
                "error": str(e),
                "processing_time": time.time() - start_time
            }
    
    def test_correction(self):
        """æ ¡æ­£æ©Ÿèƒ½ã®åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆ"""
        
        test_cases = [
            # åŸºæœ¬çš„ãªèª¤å­—
            "ä»Šæ—¥ã‚ã¨ã¦ã‚‚ã‚ˆã„å¤©æ°—ã§ã™ã­ã€‚",
            "ã“ã‚“ã«ã¡ã‚ã€å…ƒæ°—ã§ã™ã‹ï¼Ÿ",
            
            # æ¼¢å­—ã®èª¤ç”¨
            "ä¼šè­°ã®æ¬¡å¼Ÿã¯æ˜æ—¥ã®åˆå¾Œï¼“æ™‚ã‹ã‚‰ã§ã™ã€‚",
            "ã“ã®å•†å“ã®å€‹å®¢æº€è¶³åº¦ã¯é«˜ã„ã§ã™ã€‚",
            "é›°æ„æ°—ãŒã¨ã¦ã‚‚ã‚ˆã„ã§ã™ã­ã€‚",
            
            # ã‚«ã‚¿ã‚«ãƒŠèª¤å­—
            "ã‚·ãƒ¥ãƒŸãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®çµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
            "ã‚³ãƒŸãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å–ã£ã¦é€²ã‚ã¾ã—ã‚‡ã†ã€‚",
            "ãƒãƒƒãƒ†ãƒªã®äº¤æ›ãŒå¿…è¦ã§ã™ã€‚",
            
            # æ–‡å­—ç¨®ãƒ»è¨˜å·
            "ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼ï¼ï¼",
            "ã‚·ã‚¹ãƒ†ãƒ ã®ã€€ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã€€ãŒã€€å®Œäº†ã—ã¾ã—ãŸ",
            
            # æ­£ã—ã„æ–‡ï¼ˆå¤‰æ›´ä¸è¦ï¼‰
            "æœ¬æ—¥ã¯ãŠå¿™ã—ã„ä¸­ã€ãŠæ™‚é–“ã‚’ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚",
            "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®é€²æ—ã«ã¤ã„ã¦å ±å‘Šã„ãŸã—ã¾ã™ã€‚",
        ]
        
        print("\nğŸ“ Qwen3æ—¥æœ¬èªæ ¡æ­£ãƒ†ã‚¹ãƒˆ")
        print("=" * 70)
        
        total_tests = len(test_cases)
        successful_corrections = 0
        total_time = 0
        changed_count = 0
        
        for i, test_text in enumerate(test_cases, 1):
            print(f"\n--- ãƒ†ã‚¹ãƒˆ {i:2d}/{total_tests} ---")
            result = self.correct_text(test_text)
            
            print(f"ä¿®æ­£å‰: ã€Œ{result['original']}ã€")
            print(f"ä¿®æ­£å¾Œ: ã€Œ{result['corrected']}ã€")
            
            if result['success']:
                if result['changed']:
                    print("âœ… ä¿®æ­£é©ç”¨")
                    successful_corrections += 1
                    changed_count += 1
                else:
                    print("âšª ä¿®æ­£ä¸è¦ï¼ˆé©åˆ‡ãªåˆ¤æ–­ï¼‰")
                print(f"å‡¦ç†æ™‚é–“: {result['processing_time']:.2f}ç§’")
                total_time += result['processing_time']
            else:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")
        
        # çµæœã‚µãƒãƒªãƒ¼
        print(f"\nğŸ“Š Qwen3æ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ  ãƒ†ã‚¹ãƒˆçµæœ")
        print("=" * 50)
        print(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {total_tests}")
        print(f"æˆåŠŸå‡¦ç†æ•°: {total_tests - sum(1 for i in range(total_tests) if not test_cases)}")
        print(f"ä¿®æ­£é©ç”¨æ•°: {changed_count}")
        print(f"ä¿®æ­£é©ç”¨ç‡: {changed_count/total_tests*100:.1f}%")
        print(f"å¹³å‡å‡¦ç†æ™‚é–“: {total_time/total_tests:.2f}ç§’")
        print(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: Qwen3-8B-Instruct (4bité‡å­åŒ–)")
        
        return successful_corrections, total_tests

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ Qwen3-8B-Instruct æ—¥æœ¬èªæ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ")
    print("=" * 70)
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤º
    if torch.cuda.is_available():
        print(f"âœ… GPUåˆ©ç”¨å¯èƒ½: {torch.cuda.get_device_name(0)}")
        print(f"   GPU ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        print(f"   PyTorchç‰ˆ: {torch.__version__}")
    else:
        print("âš ï¸  CPUä½¿ç”¨ï¼ˆå‡¦ç†æ™‚é–“ãŒé•·ããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰")
    
    try:
        # Qwen3æ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        corrector = Qwen3TextCorrector()
        
        if corrector.model and corrector.tokenizer:
            # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            successful, total = corrector.test_correction()
            
            print(f"\nğŸ¯ æœ€çµ‚è©•ä¾¡")
            print("=" * 30)
            print(f"âœ… Qwen3-8B-Instruct (4bité‡å­åŒ–)")
            print(f"   - é«˜æ€§èƒ½å¤šè¨€èªãƒ¢ãƒ‡ãƒ«")
            print(f"   - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡: 4bité‡å­åŒ–")
            print(f"   - å•†ç”¨åˆ©ç”¨: ç¢ºèªè¦ï¼ˆãƒ©ã‚¤ã‚»ãƒ³ã‚¹æ¬¡ç¬¬ï¼‰")
            print(f"   - å‡¦ç†æ€§èƒ½: GPUæœ€é©åŒ–")
            
            # çµ±åˆæ¨å¥¨
            print(f"\nğŸ’¡ çµ±åˆæ¨å¥¨åº¦: â­â­â­â­â­")
            print(f"   ç†ç”±: é«˜ç²¾åº¦ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã€æ—¥æœ¬èªå¯¾å¿œå„ªç§€")
            
        else:
            print("âŒ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
            print("ğŸ’¡ å¯¾å‡¦æ³•:")
            print("   1. transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’æœ€æ–°ç‰ˆã«æ›´æ–°")
            print("   2. ã‚ˆã‚Šè»½é‡ãªãƒ¢ãƒ‡ãƒ«ï¼ˆQwen2.5-7Bï¼‰ã‚’è©¦ç”¨")
            print("   3. CPUãƒ¢ãƒ¼ãƒ‰ã§ã®å®Ÿè¡Œ")
            
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        print("\nğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢ä¸­...")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        print("âœ… ã‚¯ãƒªã‚¢å®Œäº†")

if __name__ == "__main__":
    main()