#!/usr/bin/env python3
"""
æ—¥æœ¬èªèª¤å­—è¨‚æ­£ç”¨LLMãƒ¢ãƒ‡ãƒ«ã®å‹•ä½œæ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å•†ç”¨åˆ©ç”¨å¯èƒ½ãªé«˜å“è³ªãƒ¢ãƒ‡ãƒ«ã®é¸å®šã¨ãƒ†ã‚¹ãƒˆ
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import time
import gc

def test_gpu_availability():
    """GPUåˆ©ç”¨å¯èƒ½æ€§ã¨ãƒ¡ãƒ¢ãƒªç¢ºèª"""
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        print(f"âœ… CUDAåˆ©ç”¨å¯èƒ½: {device_count}å€‹ã®GPU")
        for i in range(device_count):
            properties = torch.cuda.get_device_properties(i)
            total_memory = properties.total_memory / 1024**3
            print(f"   GPU {i}: {properties.name}, {total_memory:.1f}GB")
        return True
    else:
        print("âš ï¸  GPUåˆ©ç”¨ä¸å¯ã€CPUã‚’ä½¿ç”¨")
        return False

def test_model_loading(model_name, use_quantization=True):
    """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ"""
    print(f"\nğŸ”„ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_name}")
    start_time = time.time()
    
    try:
        # GPUåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # é‡å­åŒ–è¨­å®šï¼ˆGPUåˆ©ç”¨æ™‚ã®ãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
        model_kwargs = {
            "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
            "device_map": "auto" if device == "cuda" else None,
        }
        
        if use_quantization and device == "cuda":
            from transformers import BitsAndBytesConfig
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            model_kwargs["quantization_config"] = quantization_config
            print("   ğŸ“¦ 4bité‡å­åŒ–ã‚’ä½¿ç”¨ï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼‰")
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True,
            **model_kwargs
        )
        
        load_time = time.time() - start_time
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº† ({load_time:.1f}ç§’)")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
        if device == "cuda":
            memory_used = torch.cuda.memory_allocated() / 1024**3
            print(f"   GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_used:.1f}GB")
        
        return model, tokenizer, device
        
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None, None, None

def test_japanese_correction(model, tokenizer, device):
    """æ—¥æœ¬èªèª¤å­—è¨‚æ­£ãƒ†ã‚¹ãƒˆ"""
    
    if not model or not tokenizer:
        print("âŒ ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return
    
    print(f"\nğŸ“ æ—¥æœ¬èªèª¤å­—è¨‚æ­£ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    # ãƒ†ã‚¹ãƒˆç”¨ã®èª¤å­—ã‚’å«ã‚€æ—¥æœ¬èªæ–‡
    test_texts = [
        "ä»Šæ—¥ã¯ã¨ã¦ã‚‚ã‚ˆã„å¤©æ°—ã§ã™ã­ã€‚",  # æ­£ã—ã„æ–‡ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
        "ãã‚‡ã†ã¯ã¨ã¦ã‚‚ã‚ˆã„ã¦ã‚“ãã§ã™ã­ã€‚",  # ã²ã‚‰ãŒãªéå¤š
        "ä»Šæ—¥ã‚ã¨ã¦ã‚‚ã‚ˆã„å¤©æ°—ã§ã™ã­ã€‚",  # ã€Œã¯ã€â†’ã€Œã‚ã€èª¤å­—
        "ä¼šè­°ã®æ¬¡å¼Ÿã¯æ˜æ—¥ã®åˆå¾Œä¸‰æ™‚ã‹ã‚‰ã§ã™ã€‚",  # ã€Œæ¬¡ç¬¬ã€â†’ã€Œæ¬¡å¼Ÿã€èª¤å­—
        "ã“ã®å•†å“ã®å€‹å®¢æº€è¶³åº¦ã¯é«˜ã„ã§ã™ã€‚",  # ã€Œé¡§å®¢ã€â†’ã€Œå€‹å®¢ã€èª¤å­—
    ]
    
    correction_prompt = """ä»¥ä¸‹ã®æ—¥æœ¬èªæ–‡ã‚’è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„æ–‡ç« ã«ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚èª¤å­—è„±å­—ãŒã‚ã‚Œã°è¨‚æ­£ã—ã€ã‚ˆã‚Šé©åˆ‡ãªè¡¨ç¾ãŒã‚ã‚Œã°æ”¹å–„ã—ã¦ãã ã•ã„ã€‚

ä¿®æ­£å‰: {input_text}
ä¿®æ­£å¾Œ:"""
    
    results = []
    
    for i, text in enumerate(test_texts):
        print(f"\n--- ãƒ†ã‚¹ãƒˆ {i+1} ---")
        print(f"å…¥åŠ›: {text}")
        
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            prompt = correction_prompt.format(input_text=text)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
            if device == "cuda":
                inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # ç”Ÿæˆè¨­å®šï¼ˆå“è³ªé‡è¦–ï¼‰
            generation_config = {
                "max_new_tokens": 100,
                "temperature": 0.3,  # å‰µé€ æ€§ã‚’æŠ‘ãˆã¦ç¢ºå®Ÿæ€§é‡è¦–
                "top_p": 0.9,
                "do_sample": True,
                "repetition_penalty": 1.1,
                "pad_token_id": tokenizer.eos_token_id,
            }
            
            start_time = time.time()
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    **generation_config
                )
            
            # çµæœãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            correction = generated_text.split("ä¿®æ­£å¾Œ:")[-1].strip()
            
            generation_time = time.time() - start_time
            
            print(f"å‡ºåŠ›: {correction}")
            print(f"å‡¦ç†æ™‚é–“: {generation_time:.2f}ç§’")
            
            results.append({
                "input": text,
                "output": correction,
                "time": generation_time
            })
            
        except Exception as e:
            print(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            results.append({
                "input": text,
                "output": f"ã‚¨ãƒ©ãƒ¼: {e}",
                "time": 0
            })
    
    # çµæœè©•ä¾¡
    print(f"\nğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    avg_time = sum(r["time"] for r in results if r["time"] > 0) / len([r for r in results if r["time"] > 0])
    print(f"å¹³å‡å‡¦ç†æ™‚é–“: {avg_time:.2f}ç§’")
    
    successful_tests = len([r for r in results if not r["output"].startswith("ã‚¨ãƒ©ãƒ¼")])
    print(f"æˆåŠŸç‡: {successful_tests}/{len(results)} ({successful_tests/len(results)*100:.1f}%)")
    
    return results

def test_commercial_models():
    """å•†ç”¨åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ"""
    
    # å•†ç”¨åˆ©ç”¨å¯èƒ½ã§æ—¥æœ¬èªå¯¾å¿œãŒè‰¯å¥½ãªãƒ¢ãƒ‡ãƒ«
    models_to_test = [
        {
            "name": "microsoft/DialoGPT-medium",
            "description": "Microsoft DialoGPT (MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹)",
            "use_quantization": False  # å°å‹ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚é‡å­åŒ–ä¸è¦
        },
        {
            "name": "rinna/japanese-gpt-neox-3.6b",
            "description": "Rinnaæ—¥æœ¬èªGPT (MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹)",
            "use_quantization": True
        },
        {
            "name": "cyberagent/open-calm-7b",
            "description": "CyberAgent OpenCALM (Apache 2.0)",
            "use_quantization": True
        },
    ]
    
    print("ğŸš€ å•†ç”¨åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªLLMãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 60)
    
    # GPUç¢ºèª
    gpu_available = test_gpu_availability()
    
    best_model = None
    best_score = 0
    
    for model_info in models_to_test:
        print(f"\n{'='*20} {model_info['description']} {'='*20}")
        
        try:
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            model, tokenizer, device = test_model_loading(
                model_info["name"], 
                use_quantization=model_info["use_quantization"] and gpu_available
            )
            
            if model and tokenizer:
                # æ—¥æœ¬èªè¨‚æ­£ãƒ†ã‚¹ãƒˆ
                results = test_japanese_correction(model, tokenizer, device)
                
                # ç°¡æ˜“ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆæˆåŠŸç‡ + å‡¦ç†é€Ÿåº¦ï¼‰
                if results:
                    success_rate = len([r for r in results if not r["output"].startswith("ã‚¨ãƒ©ãƒ¼")]) / len(results)
                    avg_time = sum(r["time"] for r in results if r["time"] > 0) / max(len([r for r in results if r["time"] > 0]), 1)
                    speed_score = max(0, 1 - avg_time / 10)  # 10ç§’åŸºæº–ã§é€Ÿåº¦ã‚¹ã‚³ã‚¢
                    total_score = success_rate * 0.7 + speed_score * 0.3
                    
                    print(f"ğŸ“Š è©•ä¾¡ã‚¹ã‚³ã‚¢: {total_score:.3f} (æˆåŠŸç‡: {success_rate:.3f}, é€Ÿåº¦: {speed_score:.3f})")
                    
                    if total_score > best_score:
                        best_score = total_score
                        best_model = model_info
                
                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
                del model, tokenizer
                if gpu_available:
                    torch.cuda.empty_cache()
                gc.collect()
                
            else:
                print("âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—")
                
        except Exception as e:
            print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    
    # æ¨å¥¨ãƒ¢ãƒ‡ãƒ«è¡¨ç¤º
    if best_model:
        print(f"\nğŸ† æ¨å¥¨ãƒ¢ãƒ‡ãƒ«: {best_model['description']}")
        print(f"   ãƒ¢ãƒ‡ãƒ«å: {best_model['name']}")
        print(f"   è©•ä¾¡ã‚¹ã‚³ã‚¢: {best_score:.3f}")
    else:
        print("\nâŒ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    return best_model

if __name__ == "__main__":
    print("ğŸ¤– æ—¥æœ¬èªèª¤å­—è¨‚æ­£LLMãƒ¢ãƒ‡ãƒ«æ¤œè¨¼ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    try:
        # å•†ç”¨åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ
        best_model = test_commercial_models()
        
        if best_model:
            print(f"\nâœ… ãƒ†ã‚¹ãƒˆå®Œäº†")
            print(f"ğŸ¯ æ¨å¥¨è¨­å®š:")
            print(f"   - ãƒ¢ãƒ‡ãƒ«: {best_model['name']}")
            print(f"   - é‡å­åŒ–: {'æœ‰åŠ¹' if best_model['use_quantization'] else 'ç„¡åŠ¹'}")
            print(f"   - ç”¨é€”: æ—¥æœ¬èªèª¤å­—è¨‚æ­£ãƒ»æ–‡ç« æ”¹å–„")
        else:
            print(f"\nâŒ é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            print(f"ğŸ’¡ ä»£æ›¿æ¡ˆ: GPT-2ãƒ™ãƒ¼ã‚¹ã¾ãŸã¯T5ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œè¨")
        
    except Exception as e:
        print(f"\nâŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ğŸ’¡ å¯¾å‡¦æ³•: ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ç¢ºèªã€ãƒ¡ãƒ¢ãƒªä¸è¶³ã®è§£æ±º")