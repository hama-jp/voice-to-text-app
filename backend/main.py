#!/usr/bin/env python3
"""
éŸ³å£°ãƒ†ã‚­ã‚¹ãƒˆåŒ–API ã‚µãƒ¼ãƒãƒ¼
Whisper + æ—¥æœ¬èªæ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹é«˜å“è³ªãªéŸ³å£°æ–‡å­—èµ·ã“ã—
"""

import os
import sys
import tempfile
import time
from pathlib import Path
from typing import Optional

from faster_whisper import WhisperModel
import torch
from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks, Query
from fastapi.responses import JSONResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import uvicorn

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from text_corrector import JapaneseTextCorrector

# APIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–
app = FastAPI(
    title="éŸ³å£°ãƒ†ã‚­ã‚¹ãƒˆåŒ–API",
    description="Whisperã¨æ—¥æœ¬èªæ ¡æ­£ã‚’çµ„ã¿åˆã‚ã›ãŸé«˜å“è³ªéŸ³å£°æ–‡å­—èµ·ã“ã—ã‚µãƒ¼ãƒ“ã‚¹",
    version="1.1.0"
)

# CORSè¨­å®šï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã¨ã®é€£æºç”¨ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # æœ¬ç•ªç’°å¢ƒã§ã¯é©åˆ‡ãªã‚ªãƒªã‚¸ãƒ³ã‚’æŒ‡å®š
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
whisper_model = None
text_corrector = None
upload_dir = project_root / "uploads"
output_dir = project_root / "outputs"

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
upload_dir.mkdir(exist_ok=True)
output_dir.mkdir(exist_ok=True)

class TranscriptionResponse(BaseModel):
    """éŸ³å£°æ–‡å­—èµ·ã“ã—ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    success: bool
    transcription: str
    corrected_text: Optional[str] = None
    processing_time: float
    corrections_applied: list = []
    file_info: dict = {}

@app.on_event("startup")
async def startup_event():
    """ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚ã®åˆæœŸåŒ–"""
    global whisper_model, text_corrector
    
    print("ğŸš€ éŸ³å£°ãƒ†ã‚­ã‚¹ãƒˆåŒ–APIã‚µãƒ¼ãƒãƒ¼èµ·å‹•ä¸­...")
    
    try:
        # GPUç¢ºèª
        if torch.cuda.is_available():
            print(f"âœ… GPUåˆ©ç”¨å¯èƒ½: {torch.cuda.get_device_name(0)}")
            print(f"   GPU ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
            device = "cuda"
        else:
            print("âš ï¸  CPUä½¿ç”¨ï¼ˆGPUãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ï¼‰")
            device = "cpu"
        
        # Whisperãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        print("ğŸ”„ Whisperãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
        start_time = time.time()
        whisper_model = WhisperModel("large-v3", device=device, compute_type="float16" if device == "cuda" else "int8")
        load_time = time.time() - start_time
        print(f"âœ… Whisperãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº† ({load_time:.1f}ç§’)")
        
        # ãƒ†ã‚­ã‚¹ãƒˆæ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
        text_corrector = JapaneseTextCorrector()
        print("âœ… ãƒ†ã‚­ã‚¹ãƒˆæ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†")
        
        print("ğŸ¯ ã‚µãƒ¼ãƒãƒ¼åˆæœŸåŒ–å®Œäº†ï¼")
        
    except Exception as e:
        print(f"âŒ ã‚µãƒ¼ãƒãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        raise e

@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {
        "status": "healthy",
        "timestamp": time.time(),
        "gpu_available": torch.cuda.is_available(),
        "models_loaded": {
            "whisper": whisper_model is not None,
            "text_corrector_ready": text_corrector is not None
        }
    }

@app.post("/transcribe", response_model=TranscriptionResponse)
async def transcribe_audio(
    background_tasks: BackgroundTasks,
    audio_file: UploadFile = File(...),
    use_correction: bool = Query(True),
    correction_model: Optional[str] = Query("rinna/japanese-gpt-neox-small")
):
    """
    éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®æ–‡å­—èµ·ã“ã—
    """
    
    if not whisper_model:
        raise HTTPException(status_code=503, detail="Whisperãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
    
    start_time = time.time()
    temp_audio_path = None
    
    try:
        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        file_extension = Path(audio_file.filename).suffix.lower()
        if file_extension not in ['.mp3', '.wav', '.m4a', '.flac', '.aac']:
            raise HTTPException(
                status_code=400, 
                detail=f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„éŸ³å£°å½¢å¼: {file_extension}"
            )
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as tmp:
            tmp.write(await audio_file.read())
            temp_audio_path = tmp.name
        
        file_size = os.path.getsize(temp_audio_path)
        file_info = {
            "filename": audio_file.filename,
            "size_bytes": file_size,
            "size_mb": round(file_size / 1024 / 1024, 2),
            "format": file_extension
        }
        
        print(f"ğŸ“ å‡¦ç†é–‹å§‹: {audio_file.filename} ({file_info['size_mb']}MB)")
        
        # Whisperã§éŸ³å£°èªè­˜
        print("ğŸµ éŸ³å£°èªè­˜å‡¦ç†ä¸­...")
        whisper_start = time.time()
        
        segments, info = whisper_model.transcribe(
            temp_audio_path,
            language="ja",
            task="transcribe",
            beam_size=5,
        )
        
        transcription = "".join(segment.text for segment in segments)
        
        whisper_time = time.time() - whisper_start
        print(f"âœ… éŸ³å£°èªè­˜å®Œäº† ({whisper_time:.2f}ç§’): {len(transcription)}æ–‡å­—")
        
        # ãƒ†ã‚­ã‚¹ãƒˆæ ¡æ­£
        corrected_text = None
        corrections_applied = []
        
        if use_correction and text_corrector and transcription:
            print(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆæ ¡æ­£å‡¦ç†ä¸­ (ãƒ¢ãƒ‡ãƒ«: {correction_model})...")
            correction_start = time.time()
            
            correction_result = text_corrector.correct_text(
                transcription, 
                model_name=correction_model
            )
            
            corrected_text = correction_result["corrected"]
            corrections_applied = correction_result["changes"]
            correction_time = time.time() - correction_start
            
            print(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆæ ¡æ­£å®Œäº† ({correction_time:.3f}ç§’)")
            if corrections_applied:
                print(f"   é©ç”¨ã•ã‚ŒãŸæ ¡æ­£: {', '.join(corrections_applied)}")
        
        total_time = time.time() - start_time
        
        response = TranscriptionResponse(
            success=True,
            transcription=transcription,
            corrected_text=corrected_text,
            processing_time=total_time,
            corrections_applied=corrections_applied,
            file_info=file_info
        )
        
        background_tasks.add_task(cleanup_temp_file, temp_audio_path)
        
        print(f"ğŸ‰ å‡¦ç†å®Œäº† (ç·å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’)")
        
        return response
        
    except Exception as e:
        if temp_audio_path and os.path.exists(temp_audio_path):
            background_tasks.add_task(cleanup_temp_file, temp_audio_path)
        
        print(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")

async def cleanup_temp_file(file_path: str):
    """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    try:
        if os.path.exists(file_path):
            os.unlink(file_path)
            print(f"ğŸ—‘ï¸  ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {file_path}")
    except Exception as e:
        print(f"âš ï¸  ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )