#!/usr/bin/env python3
"""
Qwen2.5-7B-Instructãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸæ—¥æœ¬èªæ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ
4bité‡å­åŒ–ã«ã‚ˆã‚‹é«˜åŠ¹ç‡å®Ÿè£…
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import time
import gc

class QwenTextCorrector:
    """Qwen2.5-7B-Instructã‚’ä½¿ç”¨ã—ãŸæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆæ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, model_name="Qwen/Qwen2.5-7B-Instruct"):
        """
        åˆæœŸåŒ–
        
        Args:
            model_name (str): ä½¿ç”¨ã™ã‚‹Qwenãƒ¢ãƒ‡ãƒ«å
        """
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸ¤– Qwenæ—¥æœ¬èªæ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–: {model_name}")
        self._initialize_model()
    
    def _initialize_model(self):
        """4bité‡å­åŒ–ã§Qwenãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–"""
        try:
            print("ğŸ”„ Qwenãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
            start_time = time.time()
            
            # 4bité‡å­åŒ–è¨­å®šï¼ˆæœ€é©åŒ–ï¼‰
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆ4bité‡å­åŒ–ï¼‰
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                quantization_config=quantization_config,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            load_time = time.time() - start_time
            print(f"âœ… Qwenãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº† ({load_time:.1f}ç§’)")
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
            if self.device == "cuda":
                memory_used = torch.cuda.memory_allocated() / 1024**3
                print(f"   GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_used:.1f}GB")
            
            return True
            
        except Exception as e:
            print(f"âŒ Qwenãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            self.model = None
            self.tokenizer = None
            return False
    
    def correct_text(self, text, max_new_tokens=100):
        """
        Qwenã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆæ ¡æ­£
        
        Args:
            text (str): æ ¡æ­£å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
            max_new_tokens (int): æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°
            
        Returns:
            dict: æ ¡æ­£çµæœ
        """
        if not self.model or not self.tokenizer:
            return {
                "original": text,
                "corrected": text,
                "success": False,
                "error": "ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
            }
        
        start_time = time.time()
        
        try:
            # Qwenç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆï¼ˆChatå½¢å¼ï¼‰
            prompt = f"""ä»¥ä¸‹ã®æ—¥æœ¬èªæ–‡ã‚’è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„æ–‡ç« ã«ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚èª¤å­—è„±å­—ã®è¨‚æ­£ã€é©åˆ‡ãªæ¼¢å­—ã®ä½¿ç”¨ã€è‡ªç„¶ãªè¡¨ç¾ã¸ã®æ”¹å–„ã‚’è¡Œã„ã€ä¿®æ­£å¾Œã®æ–‡ç« ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

ä¿®æ­£å‰: {text}
ä¿®æ­£å¾Œ:"""
            
            # Chatå½¢å¼ã§ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰
            messages = [
                {
                    "role": "system", 
                    "content": "ã‚ãªãŸã¯æ—¥æœ¬èªã®æ–‡ç« æ ¡æ­£ã‚’å°‚é–€ã¨ã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚èª¤å­—è„±å­—ã®ä¿®æ­£ã€é©åˆ‡ãªæ¼¢å­—ã®ä½¿ç”¨ã€è‡ªç„¶ãªè¡¨ç¾ã¸ã®æ”¹å–„ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚"
                },
                {
                    "role": "user", 
                    "content": f"æ¬¡ã®æ–‡ç« ã‚’æ ¡æ­£ã—ã¦ãã ã•ã„ï¼š{text}"
                }
            ]
            
            # ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨
            text_input = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(
                text_input,
                return_tensors="pt",
                truncation=True,
                max_length=1024
            )
            
            if self.device == "cuda":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ç”Ÿæˆè¨­å®šï¼ˆå“è³ªé‡è¦–ï¼‰
            generation_config = {
                "max_new_tokens": max_new_tokens,
                "temperature": 0.3,  # å‰µé€ æ€§ã‚’æŠ‘ãˆã¦ç¢ºå®Ÿæ€§é‡è¦–
                "top_p": 0.8,
                "do_sample": True,
                "repetition_penalty": 1.1,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
            }
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **generation_config
                )
            
            # çµæœãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(
                outputs[0], 
                skip_special_tokens=True
            )
            
            # å…¥åŠ›éƒ¨åˆ†ã‚’é™¤å»ã—ã¦ä¿®æ­£éƒ¨åˆ†ã®ã¿æŠ½å‡º
            corrected_text = generated_text.split("assistant")[-1].strip()
            
            # ä¸è¦ãªå‰ç½®è©ã‚„è¨˜å·ã‚’é™¤å»
            corrected_text = corrected_text.replace("ä¿®æ­£å¾Œ:", "").strip()
            corrected_text = corrected_text.replace("ã€Œ", "").replace("ã€", "")
            corrected_text = corrected_text.split("\n")[0].strip()  # æœ€åˆã®è¡Œã®ã¿
            
            # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã¨åŒã˜å ´åˆã¯å¤‰æ›´ãªã—ã¨ã™ã‚‹
            if corrected_text == text or not corrected_text:
                corrected_text = text
            
            processing_time = time.time() - start_time
            
            return {
                "original": text,
                "corrected": corrected_text,
                "success": True,
                "processing_time": processing_time,
                "changed": corrected_text != text
            }
            
        except Exception as e:
            return {
                "original": text,
                "corrected": text,
                "success": False,
                "error": str(e),
                "processing_time": time.time() - start_time
            }
    
    def test_correction(self):
        """æ ¡æ­£æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
        
        test_cases = [
            "ä»Šæ—¥ã‚ã¨ã¦ã‚‚ã‚ˆã„å¤©æ°—ã§ã™ã­ã€‚",
            "ä¼šè­°ã®æ¬¡å¼Ÿã¯æ˜æ—¥ã®åˆå¾Œï¼“æ™‚ã‹ã‚‰ã§ã™ã€‚",
            "ã“ã®å•†å“ã®å€‹å®¢æº€è¶³åº¦ã¯é«˜ã„ã§ã™ã€‚",
            "ã‚·ãƒ¥ãƒŸãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®çµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
            "ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼ï¼ï¼",
            "ã“ã‚“ã«ã¡ã‚ã€ã„ã‹ãŒãŠéã”ã—ã§ã™ã‹ï¼Ÿï¼Ÿï¼Ÿ",
            "ã‚·ã‚¹ãƒ†ãƒ ã®ã€€ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã€€ãŒã€€å®Œäº†ã—ã¾ã—ãŸ",
            "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®é€²æ—çŠ¶æ³ã¯ã„ã‹ãŒã§ã™ã‹",
            "ã‚³ãƒŸãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å–ã£ã¦é€²ã‚ã¾ã—ã‚‡ã†",
            "é›°æ„æ°—ãŒã¨ã¦ã‚‚ã‚ˆã„ã§ã™ã­",
        ]
        
        print("\nğŸ“ Qwenæ—¥æœ¬èªæ ¡æ­£ãƒ†ã‚¹ãƒˆ")
        print("=" * 60)
        
        total_tests = len(test_cases)
        successful_corrections = 0
        total_time = 0
        
        for i, test_text in enumerate(test_cases, 1):
            print(f"\n--- ãƒ†ã‚¹ãƒˆ {i}/{total_tests} ---")
            result = self.correct_text(test_text)
            
            print(f"ä¿®æ­£å‰: ã€Œ{result['original']}ã€")
            print(f"ä¿®æ­£å¾Œ: ã€Œ{result['corrected']}ã€")
            
            if result['success']:
                if result['changed']:
                    print("âœ… ä¿®æ­£é©ç”¨")
                    successful_corrections += 1
                else:
                    print("âšª ä¿®æ­£ä¸è¦")
                print(f"å‡¦ç†æ™‚é–“: {result['processing_time']:.2f}ç§’")
                total_time += result['processing_time']
            else:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")
        
        # çµæœã‚µãƒãƒªãƒ¼
        print(f"\nğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
        print(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {total_tests}")
        print(f"ä¿®æ­£é©ç”¨æ•°: {successful_corrections}")
        print(f"ä¿®æ­£ç‡: {successful_corrections/total_tests*100:.1f}%")
        print(f"å¹³å‡å‡¦ç†æ™‚é–“: {total_time/total_tests:.2f}ç§’")
        
        return successful_corrections, total_tests

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ Qwen2.5-7B-Instruct æ—¥æœ¬èªæ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    # GPUç¢ºèª
    if torch.cuda.is_available():
        print(f"âœ… GPUåˆ©ç”¨å¯èƒ½: {torch.cuda.get_device_name(0)}")
        print(f"   GPU ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("âš ï¸  CPUä½¿ç”¨ï¼ˆå‡¦ç†æ™‚é–“ãŒé•·ããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰")
    
    try:
        # Qwenæ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        corrector = QwenTextCorrector()
        
        if corrector.model and corrector.tokenizer:
            # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            successful, total = corrector.test_correction()
            
            print(f"\nâœ… ãƒ†ã‚¹ãƒˆå®Œäº†")
            print(f"ğŸ¯ Qwen2.5-7B-Instruct (4bité‡å­åŒ–)")
            print(f"   - ä¿®æ­£æˆåŠŸç‡: {successful/total*100:.1f}%")
            print(f"   - 4bité‡å­åŒ–ã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–")
            print(f"   - å¤šè¨€èªå¯¾å¿œãƒ»æ—¥æœ¬èªæœ€é©åŒ–")
            
        else:
            print("âŒ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

if __name__ == "__main__":
    main()