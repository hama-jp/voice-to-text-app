#!/usr/bin/env python3
"""
åˆ©ç”¨å¯èƒ½ãªQwenãƒ¢ãƒ‡ãƒ«ã®ç¢ºèªã¨æœ€é©ãƒ¢ãƒ‡ãƒ«é¸æŠ
"""

import requests
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import time

def check_available_qwen_models():
    """åˆ©ç”¨å¯èƒ½ãªQwenãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèª"""
    
    print("ğŸ” åˆ©ç”¨å¯èƒ½ãªQwenãƒ¢ãƒ‡ãƒ«ç¢ºèªä¸­...")
    
    # æ¨å¥¨é †ã«ç¢ºèªã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆ
    candidate_models = [
        "Qwen/Qwen2.5-7B-Instruct",      # æœ€æ–°ã®å®‰å®šç‰ˆ
        "Qwen/Qwen2-7B-Instruct",        # å‰ä¸–ä»£ã®å®‰å®šç‰ˆ
        "Qwen/Qwen1.5-7B-Chat",          # è»½é‡ç‰ˆ
        "Qwen/Qwen-7B-Chat",             # ã‚ªãƒªã‚¸ãƒŠãƒ«ç‰ˆ
        "Qwen/CodeQwen1.5-7B-Chat",      # ã‚³ãƒ¼ãƒ‰ç‰¹åŒ–ï¼ˆæ–‡ç« ã‚‚å¯¾å¿œï¼‰
    ]
    
    available_models = []
    
    for model_name in candidate_models:
        print(f"\nğŸ”„ ç¢ºèªä¸­: {model_name}")
        
        try:
            # ãƒ˜ãƒƒãƒ‰ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ãƒ¢ãƒ‡ãƒ«å­˜åœ¨ç¢ºèª
            url = f"https://huggingface.co/{model_name}"
            response = requests.head(url, timeout=10)
            
            if response.status_code == 200:
                print(f"  âœ… åˆ©ç”¨å¯èƒ½")
                
                # ç°¡æ˜“çš„ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
                try:
                    tokenizer = AutoTokenizer.from_pretrained(
                        model_name, 
                        trust_remote_code=True
                    )
                    print(f"  âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿æˆåŠŸ")
                    available_models.append({
                        "name": model_name,
                        "status": "ready",
                        "description": get_model_description(model_name)
                    })
                    del tokenizer  # ãƒ¡ãƒ¢ãƒªç¯€ç´„
                except Exception as e:
                    print(f"  âš ï¸  ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
                    available_models.append({
                        "name": model_name,
                        "status": "tokenizer_issue",
                        "description": get_model_description(model_name)
                    })
            else:
                print(f"  âŒ ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯ (HTTP {response.status_code})")
                
        except Exception as e:
            print(f"  âŒ ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
    
    return available_models

def get_model_description(model_name):
    """ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜ã‚’å–å¾—"""
    descriptions = {
        "Qwen/Qwen2.5-7B-Instruct": "æœ€æ–°ç‰ˆãƒ»æ—¥æœ¬èªå¯¾å¿œå¼·åŒ–ãƒ»å•†ç”¨åˆ©ç”¨å¯èƒ½",
        "Qwen/Qwen2-7B-Instruct": "å®‰å®šç‰ˆãƒ»å¤šè¨€èªå¯¾å¿œãƒ»å•†ç”¨åˆ©ç”¨å¯èƒ½", 
        "Qwen/Qwen1.5-7B-Chat": "è»½é‡ç‰ˆãƒ»ãƒãƒ£ãƒƒãƒˆæœ€é©åŒ–",
        "Qwen/Qwen-7B-Chat": "ã‚ªãƒªã‚¸ãƒŠãƒ«ç‰ˆãƒ»å®Ÿç¸¾è±Šå¯Œ",
        "Qwen/CodeQwen1.5-7B-Chat": "ã‚³ãƒ¼ãƒ‰ç‰¹åŒ–ãƒ»æ–‡ç« æ ¡æ­£ã‚‚å¯¾å¿œ",
    }
    return descriptions.get(model_name, "è©³ç´°ä¸æ˜")

def test_qwen_model(model_name):
    """é¸æŠã•ã‚ŒãŸQwenãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ"""
    
    print(f"\nğŸ§ª {model_name} ã®å‹•ä½œãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    try:
        start_time = time.time()
        
        # 4bité‡å­åŒ–è¨­å®š
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        print("ğŸ”„ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        
        # ãƒ¢ãƒ‡ãƒ«ï¼ˆ4bité‡å­åŒ–ï¼‰
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        load_time = time.time() - start_time
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº† ({load_time:.1f}ç§’)")
        
        # GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated() / 1024**3
            print(f"ğŸ“Š GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_used:.1f}GB")
        
        # ç°¡å˜ãªæ ¡æ­£ãƒ†ã‚¹ãƒˆ
        test_text = "ä»Šæ—¥ã‚ã¨ã¦ã‚‚ã‚ˆã„å¤©æ°—ã§ã™ã­ã€‚"
        
        messages = [
            {"role": "system", "content": "æ—¥æœ¬èªæ–‡ç« ã‚’æ ¡æ­£ã—ã¦ãã ã•ã„ã€‚"},
            {"role": "user", "content": f"æ¬¡ã®æ–‡ç« ã‚’æ ¡æ­£ã—ã¦ãã ã•ã„ï¼š{test_text}"}
        ]
        
        # ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨
        if hasattr(tokenizer, 'apply_chat_template'):
            text_input = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            text_input = f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {test_text}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:"
        
        inputs = tokenizer(text_input, return_tensors="pt", truncation=True)
        
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}
        
        # ç”Ÿæˆãƒ†ã‚¹ãƒˆ
        print("ğŸ¯ æ ¡æ­£ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        generation_start = time.time()
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                temperature=0.3,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        generation_time = time.time() - generation_start
        
        # çµæœãƒ‡ã‚³ãƒ¼ãƒ‰
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        print(f"âœ… æ ¡æ­£ãƒ†ã‚¹ãƒˆå®Œäº† ({generation_time:.2f}ç§’)")
        print(f"ğŸ“ å…¥åŠ›: {test_text}")
        print(f"ğŸ“ å‡ºåŠ›: {generated_text[-100:]}")  # æœ€å¾Œã®100æ–‡å­—
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        del model, tokenizer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸš€ Qwenãƒ¢ãƒ‡ãƒ«ç¢ºèªãƒ»é¸æŠã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 50)
    
    # GPUç¢ºèª
    if torch.cuda.is_available():
        print(f"âœ… GPU: {torch.cuda.get_device_name(0)}")
        print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("âš ï¸  CPUä½¿ç”¨")
    
    # åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ç¢ºèª
    available_models = check_available_qwen_models()
    
    if not available_models:
        print("âŒ åˆ©ç”¨å¯èƒ½ãªQwenãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    print(f"\nğŸ“‹ åˆ©ç”¨å¯èƒ½ãªQwenãƒ¢ãƒ‡ãƒ« ({len(available_models)}å€‹)")
    print("=" * 60)
    
    for i, model_info in enumerate(available_models, 1):
        status_icon = "âœ…" if model_info["status"] == "ready" else "âš ï¸"
        print(f"{i}. {status_icon} {model_info['name']}")
        print(f"   {model_info['description']}")
    
    # æœ€é©ãƒ¢ãƒ‡ãƒ«æ¨å¥¨
    if available_models:
        recommended = available_models[0]  # æœ€åˆã®ãƒ¢ãƒ‡ãƒ«ï¼ˆå„ªå…ˆåº¦é †ï¼‰
        print(f"\nğŸ¯ æ¨å¥¨ãƒ¢ãƒ‡ãƒ«: {recommended['name']}")
        print(f"   ç†ç”±: {recommended['description']}")
        
        # æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ
        print(f"\nğŸ§ª æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ...")
        success = test_qwen_model(recommended['name'])
        
        if success:
            print(f"\nâœ… çµ±åˆæ¨å¥¨")
            print(f"ğŸ¯ text_corrector.py ã§ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«:")
            print(f"   model_name = '{recommended['name']}'")
        else:
            print(f"\nâš ï¸  ä»£æ›¿ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨ãŒå¿…è¦")

if __name__ == "__main__":
    main()